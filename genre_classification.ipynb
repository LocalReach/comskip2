{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "from tensorflow import keras\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc(wav_file_path):\n",
    "    y, sr = librosa.load(wav_file_path, offset=0, duration=30)\n",
    "    mfcc = numpy.array(librosa.feature.mfcc(y=y, sr=sr))\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_melspectrogram(wav_file_path):\n",
    "    y, sr = librosa.load(wav_file_path, offset=0, duration=30)\n",
    "    melspectrogram = numpy.array(librosa.feature.melspectrogram(y=y, sr=sr))\n",
    "    return melspectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chroma_vector(wav_file_path):\n",
    "    y, sr = librosa.load(wav_file_path)\n",
    "    chroma = numpy.array(librosa.feature.chroma_stft(y=y, sr=sr))\n",
    "    return chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tonnetz(wav_file_path):\n",
    "    y, sr = librosa.load(wav_file_path)\n",
    "    tonnetz = numpy.array(librosa.feature.tonnetz(y=y, sr=sr))\n",
    "    return tonnetz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(file_path):\n",
    "    # Extracting MFCC feature\n",
    "    mfcc = get_mfcc(file_path)\n",
    "    mfcc_mean = mfcc.mean(axis=1)\n",
    "    mfcc_min = mfcc.min(axis=1)\n",
    "    mfcc_max = mfcc.max(axis=1)\n",
    "    mfcc_feature = numpy.concatenate((mfcc_mean, mfcc_min, mfcc_max))\n",
    "\n",
    "    # Extracting Mel Spectrogram feature\n",
    "    melspectrogram = get_melspectrogram(file_path)\n",
    "    melspectrogram_mean = melspectrogram.mean(axis=1)\n",
    "    melspectrogram_min = melspectrogram.min(axis=1)\n",
    "    melspectrogram_max = melspectrogram.max(axis=1)\n",
    "    melspectrogram_feature = numpy.concatenate((melspectrogram_mean, melspectrogram_min, melspectrogram_max))\n",
    "\n",
    "    # Extracting chroma vector feature\n",
    "    chroma = get_chroma_vector(file_path)\n",
    "    chroma_mean = chroma.mean(axis=1)\n",
    "    chroma_min = chroma.min(axis=1)\n",
    "    chroma_max = chroma.max(axis=1)\n",
    "    chroma_feature = numpy.concatenate((chroma_mean, chroma_min, chroma_max))\n",
    "\n",
    "    # Extracting tonnetz feature\n",
    "    tntz = get_tonnetz(file_path)\n",
    "    tntz_mean = tntz.mean(axis=1)\n",
    "    tntz_min = tntz.min(axis=1)\n",
    "    tntz_max = tntz.max(axis=1)\n",
    "    tntz_feature = numpy.concatenate((tntz_mean, tntz_min, tntz_max)) \n",
    "    \n",
    "    feature = numpy.concatenate((chroma_feature, melspectrogram_feature, mfcc_feature, tntz_feature))\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating features for genre: classical\n",
      "Calculating features for genre: country\n",
      "Calculating features for genre: hiphop\n",
      "Calculating features for genre: jazz\n",
      "Calculating features for genre: rock\n"
     ]
    }
   ],
   "source": [
    "# Calculating features for the full dataset\n",
    "directory = 'Music Data'\n",
    "genres = ['classical', 'country', 'hiphop', 'jazz', 'rock']\n",
    "features = []\n",
    "labels = []\n",
    "for genre in genres:\n",
    "    print(\"Calculating features for genre: \" + genre)\n",
    "    for file in os.listdir(directory + \"/\" + genre):\n",
    "        file_path = directory + '/' + genre + '/' + file\n",
    "\n",
    "        features.append(get_feature(file_path))\n",
    "        label = genres.index(genre)\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutations = numpy.random.permutation(300)\n",
    "features = numpy.array(features)[permutations]\n",
    "labels = numpy.array(labels)[permutations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the dataset into training, validation and testing parts: 60%, 20% and 20% respectively\n",
    "features_train = features[0:180]\n",
    "labels_train = labels[0:180]\n",
    "\n",
    "features_val = features[180:240]\n",
    "labels_val = labels[180:240]\n",
    "\n",
    "features_test = features[240:300]\n",
    "labels_test = labels[240:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 211.2534 - sparse_categorical_accuracy: 0.4192 - val_loss: 31.7849 - val_sparse_categorical_accuracy: 0.7167\n",
      "Epoch 2/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 13.3204 - sparse_categorical_accuracy: 0.8134 - val_loss: 30.4228 - val_sparse_categorical_accuracy: 0.7833\n",
      "Epoch 3/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.5498 - sparse_categorical_accuracy: 0.8835 - val_loss: 13.9466 - val_sparse_categorical_accuracy: 0.8333\n",
      "Epoch 4/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 17.6305 - sparse_categorical_accuracy: 0.7784 - val_loss: 7.2123 - val_sparse_categorical_accuracy: 0.8833\n",
      "Epoch 5/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3.0470 - sparse_categorical_accuracy: 0.9623 - val_loss: 9.8508 - val_sparse_categorical_accuracy: 0.8833\n",
      "Epoch 6/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 3.2072 - sparse_categorical_accuracy: 0.8748 - val_loss: 41.1715 - val_sparse_categorical_accuracy: 0.7500\n",
      "Epoch 7/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.4154 - sparse_categorical_accuracy: 0.9375 - val_loss: 12.1225 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 8/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.3089 - sparse_categorical_accuracy: 0.9927 - val_loss: 10.8116 - val_sparse_categorical_accuracy: 0.8833\n",
      "Epoch 9/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.4509 - sparse_categorical_accuracy: 0.9943 - val_loss: 22.8246 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 10/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 38.3739 - sparse_categorical_accuracy: 0.7343 - val_loss: 16.9895 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 11/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.1519 - sparse_categorical_accuracy: 0.9486 - val_loss: 6.0277 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 12/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.6487 - sparse_categorical_accuracy: 0.9468 - val_loss: 7.8902 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 13/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.4271e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.8942 - val_sparse_categorical_accuracy: 0.9000\n",
      "Epoch 14/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.6955e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.8985 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 15/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.6427e-07 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.9074 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 16/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.0030e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.9116 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 17/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 8.4043e-07 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.9171 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 18/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 2.1533e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.9222 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 19/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.7851e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.9279 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 20/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 6.0253e-07 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.9348 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 21/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 2.5054e-07 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.9428 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 22/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.8076e-07 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.9507 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 23/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.2999e-07 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.9638 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 24/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.3356e-07 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.9693 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 25/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 7.2634e-08 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.9809 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 26/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 7.6847e-08 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.9858 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 27/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.4977e-08 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.9912 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 28/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 9.7141e-08 - sparse_categorical_accuracy: 1.0000 - val_loss: 7.9965 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 29/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 9.5589e-08 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0005 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 30/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 9.1872e-08 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0067 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 31/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.1226e-08 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0114 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 32/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.3040e-08 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0240 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 33/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.7390e-08 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0274 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 34/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.5229e-08 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0254 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 35/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 4.2087e-08 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0293 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 36/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.7988e-09 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0403 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 37/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.7855e-08 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0419 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 38/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.7331e-08 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0440 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 39/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.0337e-09 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0449 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 40/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.9886e-08 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0466 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 41/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.4085e-09 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0419 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 42/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.2586e-08 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0463 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 43/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.1845e-09 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0543 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 44/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.2329e-08 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0556 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 45/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 9.4995e-09 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0590 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 46/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5.0203e-09 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0576 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 47/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.9825e-09 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0569 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 48/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.6477e-09 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0567 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 49/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.8360e-09 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0533 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 50/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.9448e-09 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0545 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 51/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.0134e-09 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0512 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 52/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 3.5035e-09 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0525 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 53/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2818e-09 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0571 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 54/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.7453e-09 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0583 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 55/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.9270e-09 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0595 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 56/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.3061e-09 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0578 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 57/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.4192e-09 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0564 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 58/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 2.8206e-09 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0583 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 59/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.9602e-09 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0574 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 60/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.5149e-10 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0551 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 61/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.4858e-09 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0578 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 62/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.0046e-09 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0571 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 63/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6941e-09 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0564 - val_sparse_categorical_accuracy: 0.9167\n",
      "Epoch 64/64\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.0046e-09 - sparse_categorical_accuracy: 1.0000 - val_loss: 8.0563 - val_sparse_categorical_accuracy: 0.9167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x24c37ef8d30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "inputs = keras.Input(shape=(498,), name=\"feature\")\n",
    "x = keras.layers.Dense(300, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "x = keras.layers.Dense(200, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = keras.layers.Dense(3, activation=\"softmax\", name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(\n",
    "    # Optimizer\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "model.fit(x=features_train,y=labels_train,verbose=1,validation_data=(features_val, labels_val), epochs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.00000238418579%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x=features_test,y=labels_test, verbose=0)\n",
    "print('Accuracy: ' + str(score[1]*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\Desktop\\comskip2\\.venv\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='audio_sample_30s.wav'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = 30000\n",
    "t2 = 60000\n",
    "waveFile = AudioSegment.from_file(\"sample_audio_full.wav\")\n",
    "waveFile = waveFile[t1:t2]\n",
    "waveFile.export('audio_sample_30s.wav', format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hiphop'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"audio_sample_30s.wav\"\n",
    "feature = get_feature(file_path)\n",
    "y = model.predict(feature.reshape(1,498))\n",
    "ind = numpy.argmax(y)\n",
    "genres[ind]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
